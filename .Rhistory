mean.x.bar.theory <- mu
mean.x.bar.theory
mean.x.bar.sim <- mean(x.bar.sample)
mean.x.bar.sim
var.x.bar.theory <- sig2/n
var.x.bar.theory
var.x.bar.sim <- var(x.bar.sample)
var.x.bar.sim
hist(x.bar.sample, breaks=15, xlab="sample mean", main="Simulated sampling distribution of the sample mean")
table(ci.sample) / n.rep
n.rep
table(ci.sample)
n <- 5
t.crit <- qt(alpha/2, df=n-1, lower.tail=FALSE)
x.bar.sample <- numeric(length=n.rep)
ci.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
x <- rchisq(n, df=3)
x.bar <- mean(x)
m.err <- t.crit*sd(x)/sqrt(n)
x.bar.sample[rep] <- mean(x)
ci.sample[rep] <- (abs(x.bar - mu) <= m.err)
}
mean.x.bar.theory <- mu
mean.x.bar.theory
mean.x.bar.sim <- mean(x.bar.sample)
mean.x.bar.sim
var.x.bar.theory <- sig2/n
var.x.bar.theory
var.x.bar.sim <- var(x.bar.sample)
var.x.bar.sim
hist(x.bar.sample, breaks=15, xlab="sample mean", main="Simulated sampling distribution of the sample mean")
table(ci.sample) / n.rep
n <- 25
t.crit <- qt(alpha/2, df=n-1, lower.tail=FALSE)
x.bar.sample <- numeric(length=n.rep)
ci.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
x <- rnorm(n, mean=3, sd=sqrt(6))
x.bar <- mean(x)
m.err <- t.crit*sd(x)/sqrt(n)
x.bar.sample[rep] <- mean(x)
ci.sample[rep] <- (abs(x.bar - mu) <= m.err)
}
mean.x.bar.theory <- mu
mean.x.bar.theory
mean.x.bar.sim <- mean(x.bar.sample)
mean.x.bar.sim
var.x.bar.theory <- sig2/n
var.x.bar.theory
var.x.bar.sim <- var(x.bar.sample)
var.x.bar.sim
table(ci.sample) / n.rep
n <- 25
t.crit <- qt(alpha/2, df=n-1, lower.tail=FALSE)
x.bar.sample <- numeric(length=n.rep)
ci.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
x <- rchisq(n, df=3)
x.bar <- mean(x)
m.err <- t.crit*sd(x)/sqrt(n)
x.bar.sample[rep] <- mean(x)
ci.sample[rep] <- (abs(x.bar - mu) <= m.err)
}
mean.x.bar.theory <- mu
mean.x.bar.theory
mean.x.bar.sim <- mean(x.bar.sample)
mean.x.bar.sim
# Theoretical and simulated variance of the sampling     |
# distribution                                           |
var.x.bar.theory <- sig2/n
var.x.bar.theory
var.x.bar.sim <- var(x.bar.sample)
var.x.bar.sim
table(ci.sample) / n.rep
#========================================================#
# STAT 6021: Linear Models for Data Science              |
# Hands-on team laboratory exercise for unit 1           |
# Team solution                                          |
#========================================================#
#
# List the contributions of each team member:
# <Name of member 1>: <Contribution of member 1>
# <Name of member 2>: <Contribution of member 2>
# etc.
#
# -------------------------------------------------------+
# Problem 1:                                             |
# -------------------------------------------------------+
#                                                        |
# Part A:                                                |
n <- 5
x <- rnorm(n, mean=3, sd=sqrt(6))
mu <- 3           # population mean                      |
|
n.rep <- 500      # stores the number of repetitions of  |
# the simulation.                      |
alpha <- 0.05     # defines the stated confidence level  |
# of the confidence interval           |
t.crit <- qt(alpha/2, df=n-1, lower.tail=FALSE)
x.median.sample <- numeric(length=n.rep)
ci.sample <- logical(length=n.rep)
for (rep in 1:n.rep) {
x <- rnorm(n, mean=3, sd=sqrt(6))
x.median <- median(x)
m.err <- t.crit*sd(x)/sqrt(n)
x.median.sample[rep] <- median(x)
ci.sample[rep] <- (abs(x.median - mu) <= m.err)
}
pop.median <- qnorm(0.5, mean=3, sd=sqrt(6))
pop.median
mean.x.median.sim <- mean(x.median.sample)
mean.x.median.sim
# Variance of the sampling distribution
var(x.median.sample)
# population variance
sig2
sqrt(sig2)
setwd("C:/Users/Fang/Downloads/f18/6018/Kaggle/house")
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = training)
summary(lm1)$r.squared
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
q()
setwd("C:/Users/Fang/Downloads/f18/6018/Kaggle")
setwd("C:/Users/Fang/Downloads/f18/6018/Kaggle/house")
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = training)
summary(lm1)$r.squared
# 0.6476122
# a moderately high value.
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# Something that I think we can further explore:
# factorize all categorical variables and study their strengths as predictors
# check outliers / NAs, then either remove them or populate them with speculations
# correlation analysis for specific predictor variables
# check if there is systematic way of doing this "Feature Engineering". So far my understanding of it is that the term is roughly interchangeable with "Model Building", but I could be wrong :)
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = training)
summary(lm1)$r.squared
# 0.6476122
# a moderately high value.
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# Something that I think we can further explore:
# factorize all categorical variables and study their strengths as predictors
# check outliers / NAs, then either remove them or populate them with speculations
# correlation analysis for specific predictor variables
# check if there is systematic way of doing this "Feature Engineering". So far my understanding of it is that the term is roughly interchangeable with "Model Building", but I could be wrong :)
huh <- predict(lm_all, newdata = house_pred)
length(huh)
huh[1117]
huh[1117, ]
huh[1116:1118]
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea, data = training)
summary(lm1)$r.squared
# 0.6476122
# a moderately high value.
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# Something that I think we can further explore:
# factorize all categorical variables and study their strengths as predictors
# check outliers / NAs, then either remove them or populate them with speculations
# correlation analysis for specific predictor variables
# check if there is systematic way of doing this "Feature Engineering". So far my understanding of it is that the term is roughly interchangeable with "Model Building", but I could be wrong :)
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = training)
summary(lm1)$r.squared
# 0.6476122
# a moderately high value.
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# Something that I think we can further explore:
# factorize all categorical variables and study their strengths as predictors
# check outliers / NAs, then either remove them or populate them with speculations
# correlation analysis for specific predictor variables
# check if there is systematic way of doing this "Feature Engineering". So far my understanding of it is that the term is roughly interchangeable with "Model Building", but I could be wrong :)
anyNA(training$SalePrice)
anyNA(house_train$SalePrice)
house_pred[1117, ]
lm_all
predict(lm_all, newdata = house_pred[1117, ]))
predict(lm_all, newdata = house_pred[1117, ])
predict(lm_all, newdata = house_pred[1116, ])
predict(lm_all, newdata = house_pred[1118, ])
house_pred[1117, "GrLivArea"]
house_pred[1117, "GarageArea"]
house_pred[1117, "OpenPorchSF"]
house_pred[isNA(house_pred$OpenPorchSF), ]
?isNA
?is.na
house_pred_nafill <- house_pred
is.na(house_pred_nafill$OpenPorchSF) <- mean(house_train$OpenPorchSF)
house_pred_nafill[1117, ]
house_pred_nafill[1117, "OpenPorchSF"]
is.na(house_pred_nafill$GarageArea) <- mean(house_train$GarageArea)
house_pred_nafill[1117, "GarageArea"]
mean(house_train$GarageArea)
is.na(house_pred_nafill$GarageArea)
house_train$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
house_pred_nafill[1117, "GarageArea"]
house_train$GarageArea[is.na(house_pred_nafill$GarageArea)]
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
house_pred_nafill[1117, "GarageArea"]
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice. We attempt to construct a first model using these predicting variables:
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = training)
summary(lm1)$r.squared
# 0.6476122
# a moderately high value.
# filling NA values
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + OpenPorchSF, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred_nafill))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# Something that I think we can further explore:
# factorize all categorical variables and study their strengths as predictors
# check outliers / NAs, then either remove them or populate them with speculations
# correlation analysis for specific predictor variables
# check if there is systematic way of doing this "Feature Engineering". So far my understanding of it is that the term is roughly interchangeable with "Model Building", but I could be wrong :)
q()
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
house_train[1, "YearBuilt"]
a = numric()
a = numeric()
a[1]
a[1] =1
a[2] =3
a
new_train <- logical()
new_pred <- logical()
for (i in 1:nrow(house_train)) {
if (house_train[i, "YearBuilt"] >= 2000) {
new_train[i] = TRUE
}
else {
new_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "YearBuilt"] >= 2000) {
new_pred[i] = TRUE
}
else {
new_pred[i] = FALSE
}
}
house_train$New <- new_train
house_pred$New <- new_pred
# OpenPorch
op_train <- logical()
op_pred <- logical()
for (i in 1:nrow(house_train)) {
if (house_train[i, "OpenPorchSF"] > 0) {
op_train[i] = TRUE
}
else {
op_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "OpenPorchSF"] > 0) {
op_pred[i] = TRUE
}
else {
op_pred[i] = FALSE
}
}
house_train$HasOpenPorch <- op_train
house_pred$HasOpenPorch <- op_pred
house_train$New <- factor(house_train$New)
house_pred$New <- factor(house_pred$New)
house_train$HasOpenPorch <- factor(house_train$HasOpenPorch)
house_pred$HasOpenPorch <- factor(house_pred$HasOpenPorch)
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + GarageArea + HasOpenPorch + New, data = training)
summary(lm1)$r.squared
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
lm_all <- lm(SalePrice ~ GrLivArea + GarageArea + HasOpenPorch + New, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred_nafill))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
q()
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
?poly
# We also factorize some other variables that proved to be significant:
# YearBuilt -> New (in or after 2000) or Old (before 2000)
new_train <- logical()
new_pred <- logical()
for (i in 1:nrow(house_train)) {
if (house_train[i, "YearBuilt"] >= 2000) {
new_train[i] = TRUE
}
else {
new_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "YearBuilt"] >= 2000) {
new_pred[i] = TRUE
}
else {
new_pred[i] = FALSE
}
}
house_train$New <- new_train
house_pred$New <- new_pred
# OpenPorch
op_train <- logical()
op_pred <- logical()
for (i in 1:nrow(house_train)) {
if (house_train[i, "OpenPorchSF"] > 0) {
op_train[i] = TRUE
}
else {
op_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "OpenPorchSF"] > 0) {
op_pred[i] = TRUE
}
else {
op_pred[i] = FALSE
}
}
house_train$HasOpenPorch <- op_train
house_pred$HasOpenPorch <- op_pred
house_train$New <- factor(house_train$New)
house_pred$New <- factor(house_pred$New)
house_train$HasOpenPorch <- factor(house_train$HasOpenPorch)
house_pred$HasOpenPorch <- factor(house_pred$HasOpenPorch)
# CV
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
lm1 <- lm(SalePrice ~ GrLivArea + poly(OverallQual, 2) + GarageArea + HasOpenPorch + New, data = training)
summary(lm1)$r.squared
# filling NA values
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
lm_all <- lm(SalePrice ~ GrLivArea + poly(OverallQual, 2) + GarageArea + HasOpenPorch + New, data = house_train)
house_output <- data.frame(house_pred$Id, predict(lm_all, newdata = house_pred_nafill))
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
q()
