for (i in (1:(length(a) - 1))) {
d = d + (a[[i]] - b[[i]]) ^ 2
}
return(sqrt(d))
}
# KNN
knn <- function(train, k, pred){
price_all <- c()
for (i in 1:nrow(pred)) {
distances =c()
saleprices = c()
for (j in 1:nrow(train)) {
distances <- c(distances, edist(train[j,]), pred[i,])
saleprices <- c(saleprices, train[j, ]$SalePrice)
}
euclidean <- data.frame(distances, saleprices)
euclidean <- euclidean[order(distances), ][1:k, ]
price_all <- c(price_all, mean(euclidean[, 2]))
}
return(price_all)
}
k_output = sqrt(nrow(house_pred))
house_output <- data.frame(house_pred$Id, knn(house_train_sub, k_output, house_pred_sub))
write.table(house_output, file = "house_nonpar.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice.
# filling NA values
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
# CV
# train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
# training <- house_train[train, ]
# testing <- house_train[-train, ]
# feature enginering
# choosing variables OverallQual, GrLivArea, GarageArea and OpenPorchSF
house_train_sub <- house_train[, c(18, 47, 63, 68)]
house_pred_sub <- house_pred[, c(18, 47, 63, 68)]
# Euclidean distance
edist <- function(a, b){
d = 0
for (i in (1:(length(a) - 1))) {
d = d + (a[[i]] - b[[i]]) ^ 2
}
return(sqrt(d))
}
# KNN
knn <- function(train, k, pred){
price_all <- c()
for (i in 1:nrow(pred)) {
distances =c()
saleprices = c()
for (j in 1:nrow(train)) {
distances <- c(distances, edist(train[j,], pred[i,]))
saleprices <- c(saleprices, train[j, ]$SalePrice)
}
euclidean <- data.frame(distances, saleprices)
euclidean <- euclidean[order(distances), ][1:k, ]
price_all <- c(price_all, mean(euclidean[, 2]))
}
return(price_all)
}
k_output = sqrt(nrow(house_pred))
house_output <- data.frame(house_pred$Id, knn(house_train_sub, k_output, house_pred_sub))
write.table(house_output, file = "house_nonpar.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
knn(house_train_sub, k_output, house_pred_sub)
q()
# sys6018-competition-house-prices
# https://www.kaggle.com/c/house-prices-advanced-regression-techniques
library(readr)
library(dplyr)
library(caret)
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# sample <- read_csv("sample_submission.csv")
# After preliminary examination of data set and its descriptions, we first study the relationship between house areas and SalePrice. Specifically, we look at variables LotArea, GrLivArea, GarageArea, OpenPorchSF and PoolArea.
attach(house_train)
par(mfrow = c(2, 3))
plot(LotArea, SalePrice)
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(PoolArea, SalePrice)
# We observe a moderately strong positive linear relationship between GrLivArea and SalePrice, OpenPorchSF and SalePrice as well as GarageArea and SalePrice.
# filling NA values
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
# CV
# train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
# training <- house_train[train, ]
# testing <- house_train[-train, ]
# feature enginering
# choosing variables OverallQual, GrLivArea, GarageArea and OpenPorchSF
house_train_sub <- house_train[, c(18, 47, 63, 68)]
house_pred_sub <- house_pred[, c(18, 47, 63, 68)]
# Euclidean distance
edist <- function(a, b){
d = 0
for (i in (1:(length(a) - 1))) {
d = d + (a[[i]] - b[[i]]) ^ 2
}
return(sqrt(d))
}
# KNN
knn <- function(train, k, pred){
price_all <- c()
for (i in 1:nrow(pred)) {
distances =c()
saleprices = c()
for (j in 1:nrow(train)) {
distances <- c(distances, edist(train[j,], pred[i,]))
saleprices <- c(saleprices, train[j, ]$SalePrice)
}
euclidean <- data.frame(distances, saleprices)
euclidean <- euclidean[order(distances), ][1:k, ]
price_all <- c(price_all, mean(euclidean[, 2]))
}
return(price_all)
}
x <- house_pred_sub[1:3, 1:3]
x
y <- house_train_sub[1:3, 1:3]
y
x <- x[, 1:2]
x
y
knn(y, 1, x)
x <- house_train_sub[, c(1:3, ncol(house_train_sub))]
x
y <- house_pred_sub[, c(1:3)]
y
sqrt(1459)
knn(x, 38, y)
warnings()
colnames(house_pred_sub)
colnames(house_train_sub)
# choosing variables OverallQual, GrLivArea, GarageArea and OpenPorchSF
house_train_sub <- house_train[, c(18, 47, 63, 68, ncol(house_train))]
colnames(house_train_sub)
k_output
k_output = sqrt(nrow(house_pred))
house_output <- data.frame(house_pred$Id, knn(house_train_sub, k_output, house_pred_sub))
head(hosue_output)
head(house_output)
head(house_output, 15)
write.table(house_output, file = "house_nonpar.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
q()
# get and plot kde with varying bandwidths
kde = sapply(eval.pts, est.kde, sample = sample, bandwidth = 0.01)
hist(sample, freq = FALSE, breaks = 100)
# Univariate
est.kde = function(x, s, b) {
kernel.inputs = (s - x) / b
kernel.values = sapply(kernel.inputs, std.norm.pdf)
est = 1 / (length(s) * b) * sum(kernel.values)
return(est)
}
sample <- c(rnorm(1000, mean = 0, sd = 1), rnorm(1000, mean = 5, sd = 1))
eval.pts <- seq(-3, 10, 0.1)
# get and plot kde with varying bandwidths
kde = sapply(eval.pts, est.kde, sample = sample, bandwidth = 0.01)
hist(sample, freq = FALSE, breaks = 100)
# Bivariate
# Univariate
est.kde = function(x, s, b) {
kernel.inputs = (s - x) / b
kernel.values = sapply(kernel.inputs, std.norm.pdf)
est = 1 / (length(s) * b) * sum(kernel.values)
return(est)
}
sample <- c(rnorm(1000, mean = 0, sd = 1), rnorm(1000, mean = 5, sd = 1))
eval.pts <- seq(-3, 10, 0.1)
# get and plot kde with varying bandwidths
kde = sapply(eval.pts, est.kde, sample = sample, bandwidth = 0.01)
hist(sample, freq = FALSE, breaks = 100)
# Bivariate
# get and plot kde with varying bandwidths
kde = sapply(eval.pts, est.kde, s = sample, b = 0.01)
q()
library(readr)
library(dplyr)
library(caret)
setwd("/home/dell/Desktop/MSDS/Fall/data mining (SYS)/all")
# Read in data
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# Data exploration
# Feature Engineering
new_train <- logical()
new_pred <- logical()
# adding a column that states whether a house is considered "new" (built after 2000) or not
for (i in 1:nrow(house_train)) {
if (house_train[i, "YearBuilt"] >= 2000) {
new_train[i] = TRUE
}
else {
new_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "YearBuilt"] >= 2000) {
new_pred[i] = TRUE
}
else {
new_pred[i] = FALSE
}
}
house_train$New <- new_train
house_pred$New <- new_pred
house_train$New <- factor(house_train$New)
house_pred$New <- factor(house_pred$New)
op_train <- logical()
op_pred <- logical()
# adding a column that states whether a house has a porch or not
for (i in 1:nrow(house_train)) {
if (house_train[i, "OpenPorchSF"] > 0) {
op_train[i] = TRUE
}
else {
op_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "OpenPorchSF"] > 0) {
op_pred[i] = TRUE
}
else {
op_pred[i] = FALSE
}
}
house_train$HasOpenPorch <- op_train
house_pred$HasOpenPorch <- op_pred
house_train$HasOpenPorch <- factor(house_train$HasOpenPorch)
house_pred$HasOpenPorch <- factor(house_pred$HasOpenPorch)
# adding a column that states whether a house has a basement or not
house_train$isBasement <- as.numeric(house_train$TotalBsmtSF > 0)
house_pred$isBasement <- as.numeric(house_pred$TotalBsmtSF > 0)
# adding a column that states whether a house has been remodeled recently (during or after 1990) or not
house_train$isnewremodel <- as.numeric(house_train$YearRemodAdd > 1990)
house_pred$isnewremodel <- as.numeric(house_pred$YearRemodAdd > 1990)
# adding a column that states whether a house has a garage or not
house_train$isgarage <- as.numeric(house_train$GarageArea > 0)
house_pred$isgarage <- as.numeric(house_pred$GarageArea > 0)
# adding a column that states whether a house has a fireplace (one or more) or not
house_train$Fireplaces <- as.numeric(house_train$Fireplaces > 0)
house_pred$Fireplaces <- as.numeric(house_pred$Fireplaces > 0)
# Normalizing some Data
house_train$QualNorm <- (OverallQual - mean(OverallQual))/sd(OverallQual)
house_train$GrLivAreaNorm <- (GrLivArea - mean(GrLivArea))/sd(GrLivArea)
# house_train$OpenPorchNorm <- as.numeric(OpenPorchSF > 0)
# house_train$PoolNorm <- as.numeric(PoolArea > 0)
# house_train$GarageNorm <- GarageArea/mean(GarageArea)
# house_train$BsmtNorm <- TotalBsmtSF/mean(TotalBsmtSF)
# house_train$new_old <- as.numeric(YearBuilt > 1995)
# house_train$LotAreaNorm <- LotArea/mean(LotArea)
house_pred$QualNorm <- (house_pred$OverallQual - mean(house_pred$OverallQual))/sd(house_pred$OverallQual)
house_pred$GrLivAreaNorm <- (house_pred$GrLivArea - mean(house_pred$GrLivArea))/sd(house_pred$GrLivArea)
# house_pred$GarageNorm <- house_pred$GarageArea/mean(house_pred$GarageArea)
# house_pred$BsmtNorm <- house_pred$TotalBsmtSF/mean(house_pred$TotalBsmtSF)
# house_pred$BsmtNorm <- house_pred$LotArea/mean(house_pred$LotArea)
# house_pred$new_old <-  as.numeric(house_pred$YearBuilt > 1995)
# testing with normalized data
norm_train_sub <- select(house_train, QualNorm, GrLivAreaNorm, Neighborhood, SalePrice)
norm_test_sub <- select(house_pred, QualNorm, GrLivAreaNorm, Neighborhood)
norm_output <- data.frame(testing, knn(norm_train_sub, norm_test_sub))
# Below are some things for testing out the model with CV
# norm_train_sub <- select(training, QualNorm, GrLivAreaNorm, OpenPorchNorm, Neighborhood, SalePrice)
# norm_test_sub <- select(testing, QualNorm, GrLivAreaNorm, OpenPorchNorm, Neighborhood)
# norm_output <- data.frame(norm_test_sub, knn(norm_train_sub, norm_test_sub))
# reg_output$reg_output_resid = abs(reg_output$SalePrice - reg_output$knn.house_train_sub..k_output..house_pred_sub.)
# norm_output$norm_output_resid = abs(norm_output$knn.norm_train_sub..norm_test_sub. - testing$SalePrice)
# mean(norm_output$norm_output_resid)
# plot(norm_output$SalePrice, norm_output$norm_output_resid)
# plots
attach(house_train)
par(mfrow = c(2, 3))
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(BedroomAbvGr, SalePrice)
plot(YearBuilt, SalePrice)
# plot(Neighborhood, SalePrice)
plot(house_train$isgarage, SalePrice)
plot(house_train$isnewremodel, SalePrice)
plot(OverallQual, SalePrice)
table(house_train$isBasement)
# Data cleaning (missing values, outliers, etc.)
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
# Rationale for the selected statistical modeling methods
# Correct implementation and use of statistical modeling methods
# Parametric
# partitioning the train dataset into training and testing sections
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
# performing parametric modeling using a linear regression model
lm1 <- lm(SalePrice ~ GrLivArea + poly(OverallQual, 2) + GarageArea + HasOpenPorch + isnewremodel + New + YearBuilt, data = training)
summary(lm1)$r.squared
# r.squared value : 0.8303971
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# non-Parametric
# Define key functions
# Euclidean distance
edist <- function(a, b){
d = 0
for (i in (1:(length(a)))) {
d = d + (a[[i]] - b[[i]]) ^ 2
}
return(sqrt(d))
}
# KNN
knn <- function(trainer, predictions){
price_all <- c()
for (i in 1:nrow(predictions)) {
distances =c()  # create some empty vectors we will fill in with necessary info
saleprices = c()
my_neighborhoods = c()
variables = length(predictions[i,])-1 # get the number of variables to be analyzed
for (j in 1:nrow(trainer)) {
distances <- c(distances, edist(trainer[j,1:variables], predictions[i,1:variables])) # store distances in a vector
saleprices <- c(saleprices, trainer[j, ]$SalePrice) # store the sale prices in a vector
my_neighborhoods <- c(my_neighborhoods, trainer[j, ]$Neighborhood) # store the neighborhood of each comparison which will be used to subset the data
}
euclidean <- data.frame(distances, saleprices, my_neighborhoods) # combine the data we stored above
euc_subset <- euclidean[predictions[i,]$Neighborhood == euclidean$my_neighborhoods,] # subset to only look at the houses which are the same neighborhood as the observation we are looking at
number_in_neighborhood = dim(euc_subset)[1] # look at how many houses are in that neighborhood set
k = sqrt(number_in_neighborhood) # use k as square root of the number of houses that match the neighborhood
euclidean_new <- euc_subset[order(euc_subset$distances), ][1:k, ] # order on euclidian distance and only take the top k
price_all <- c(price_all, mean(euclidean_new$saleprices)) # take the mean sale price of the k houses we selected and store it in the price vector
}
return(price_all)
}
# creating the ouput csv file
house_output <- data.frame(house_pred$Id, knn(norm_train_sub, norm_test_sub))
write.table(house_output, file = "house_nonpar.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
View(house_train)
setwd("C:/Users/Fang/Downloads/f18/6018/Kaggle/house")
library(readr)
library(dplyr)
library(caret)
# setwd("/home/dell/Desktop/MSDS/Fall/data mining (SYS)/all")
# Read in data
house_train <- read_csv("train.csv")
house_pred <- read_csv("test.csv")
# Data exploration
# Feature Engineering
new_train <- logical()
new_pred <- logical()
# adding a column that states whether a house is considered "new" (built after 2000) or not
for (i in 1:nrow(house_train)) {
if (house_train[i, "YearBuilt"] >= 2000) {
new_train[i] = TRUE
}
else {
new_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "YearBuilt"] >= 2000) {
new_pred[i] = TRUE
}
else {
new_pred[i] = FALSE
}
}
house_train$New <- new_train
house_pred$New <- new_pred
house_train$New <- factor(house_train$New)
house_pred$New <- factor(house_pred$New)
op_train <- logical()
op_pred <- logical()
# adding a column that states whether a house has a porch or not
for (i in 1:nrow(house_train)) {
if (house_train[i, "OpenPorchSF"] > 0) {
op_train[i] = TRUE
}
else {
op_train[i] = FALSE
}
}
for (i in 1:nrow(house_pred)) {
if (house_pred[i, "OpenPorchSF"] > 0) {
op_pred[i] = TRUE
}
else {
op_pred[i] = FALSE
}
}
house_train$HasOpenPorch <- op_train
house_pred$HasOpenPorch <- op_pred
house_train$HasOpenPorch <- factor(house_train$HasOpenPorch)
house_pred$HasOpenPorch <- factor(house_pred$HasOpenPorch)
# adding a column that states whether a house has a basement or not
house_train$isBasement <- as.numeric(house_train$TotalBsmtSF > 0)
house_pred$isBasement <- as.numeric(house_pred$TotalBsmtSF > 0)
# adding a column that states whether a house has been remodeled recently (during or after 1990) or not
house_train$isnewremodel <- as.numeric(house_train$YearRemodAdd > 1990)
house_pred$isnewremodel <- as.numeric(house_pred$YearRemodAdd > 1990)
# adding a column that states whether a house has a garage or not
house_train$isgarage <- as.numeric(house_train$GarageArea > 0)
house_pred$isgarage <- as.numeric(house_pred$GarageArea > 0)
# adding a column that states whether a house has a fireplace (one or more) or not
house_train$Fireplaces <- as.numeric(house_train$Fireplaces > 0)
house_pred$Fireplaces <- as.numeric(house_pred$Fireplaces > 0)
# Normalizing some Data
house_train$QualNorm <- (OverallQual - mean(OverallQual))/sd(OverallQual)
house_train$GrLivAreaNorm <- (GrLivArea - mean(GrLivArea))/sd(GrLivArea)
# house_train$OpenPorchNorm <- as.numeric(OpenPorchSF > 0)
# house_train$PoolNorm <- as.numeric(PoolArea > 0)
# house_train$GarageNorm <- GarageArea/mean(GarageArea)
# house_train$BsmtNorm <- TotalBsmtSF/mean(TotalBsmtSF)
# house_train$new_old <- as.numeric(YearBuilt > 1995)
# house_train$LotAreaNorm <- LotArea/mean(LotArea)
house_pred$QualNorm <- (house_pred$OverallQual - mean(house_pred$OverallQual))/sd(house_pred$OverallQual)
house_pred$GrLivAreaNorm <- (house_pred$GrLivArea - mean(house_pred$GrLivArea))/sd(house_pred$GrLivArea)
# house_pred$GarageNorm <- house_pred$GarageArea/mean(house_pred$GarageArea)
# house_pred$BsmtNorm <- house_pred$TotalBsmtSF/mean(house_pred$TotalBsmtSF)
# house_pred$BsmtNorm <- house_pred$LotArea/mean(house_pred$LotArea)
# house_pred$new_old <-  as.numeric(house_pred$YearBuilt > 1995)
# testing with normalized data
norm_train_sub <- select(house_train, QualNorm, GrLivAreaNorm, Neighborhood, SalePrice)
norm_test_sub <- select(house_pred, QualNorm, GrLivAreaNorm, Neighborhood)
norm_output <- data.frame(testing, knn(norm_train_sub, norm_test_sub))
# Below are some things for testing out the model with CV
# norm_train_sub <- select(training, QualNorm, GrLivAreaNorm, OpenPorchNorm, Neighborhood, SalePrice)
# norm_test_sub <- select(testing, QualNorm, GrLivAreaNorm, OpenPorchNorm, Neighborhood)
# norm_output <- data.frame(norm_test_sub, knn(norm_train_sub, norm_test_sub))
# reg_output$reg_output_resid = abs(reg_output$SalePrice - reg_output$knn.house_train_sub..k_output..house_pred_sub.)
# norm_output$norm_output_resid = abs(norm_output$knn.norm_train_sub..norm_test_sub. - testing$SalePrice)
# mean(norm_output$norm_output_resid)
# plot(norm_output$SalePrice, norm_output$norm_output_resid)
# plots
attach(house_train)
par(mfrow = c(2, 3))
plot(GrLivArea, SalePrice)
plot(GarageArea, SalePrice)
plot(OpenPorchSF, SalePrice)
plot(BedroomAbvGr, SalePrice)
plot(YearBuilt, SalePrice)
# plot(Neighborhood, SalePrice)
plot(house_train$isgarage, SalePrice)
plot(house_train$isnewremodel, SalePrice)
plot(OverallQual, SalePrice)
table(house_train$isBasement)
# Data cleaning (missing values, outliers, etc.)
house_pred_nafill <- house_pred
house_pred_nafill$GarageArea[is.na(house_pred_nafill$GarageArea)] <- mean(house_train$GarageArea)
# Rationale for the selected statistical modeling methods
# Correct implementation and use of statistical modeling methods
# Parametric
# partitioning the train dataset into training and testing sections
train <- createDataPartition(house_train$SalePrice, p=0.6, list=FALSE)
training <- house_train[train, ]
testing <- house_train[-train, ]
# performing parametric modeling using a linear regression model
lm1 <- lm(SalePrice ~ GrLivArea + poly(OverallQual, 2) + GarageArea + HasOpenPorch + isnewremodel + New + YearBuilt, data = training)
summary(lm1)$r.squared
# r.squared value : 0.8303971
write.table(house_output, file = "house.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
# non-Parametric
# Define key functions
# Euclidean distance
edist <- function(a, b){
d = 0
for (i in (1:(length(a)))) {
d = d + (a[[i]] - b[[i]]) ^ 2
}
return(sqrt(d))
}
# KNN
knn <- function(trainer, predictions){
price_all <- c()
for (i in 1:nrow(predictions)) {
distances =c()  # create some empty vectors we will fill in with necessary info
saleprices = c()
my_neighborhoods = c()
variables = length(predictions[i,])-1 # get the number of variables to be analyzed
for (j in 1:nrow(trainer)) {
distances <- c(distances, edist(trainer[j,1:variables], predictions[i,1:variables])) # store distances in a vector
saleprices <- c(saleprices, trainer[j, ]$SalePrice) # store the sale prices in a vector
my_neighborhoods <- c(my_neighborhoods, trainer[j, ]$Neighborhood) # store the neighborhood of each comparison which will be used to subset the data
}
euclidean <- data.frame(distances, saleprices, my_neighborhoods) # combine the data we stored above
euc_subset <- euclidean[predictions[i,]$Neighborhood == euclidean$my_neighborhoods,] # subset to only look at the houses which are the same neighborhood as the observation we are looking at
number_in_neighborhood = dim(euc_subset)[1] # look at how many houses are in that neighborhood set
k = sqrt(number_in_neighborhood) # use k as square root of the number of houses that match the neighborhood
euclidean_new <- euc_subset[order(euc_subset$distances), ][1:k, ] # order on euclidian distance and only take the top k
price_all <- c(price_all, mean(euclidean_new$saleprices)) # take the mean sale price of the k houses we selected and store it in the price vector
}
return(price_all)
}
# creating the ouput csv file
house_output <- data.frame(house_pred$Id, knn(norm_train_sub, norm_test_sub))
write.table(house_output, file = "house_nonpar.csv", row.names=F, col.names=c("Id", "SalePrice"), sep=",")
View(house_train)
head(house_output)
q()
